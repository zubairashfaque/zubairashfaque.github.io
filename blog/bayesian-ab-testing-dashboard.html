<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Bayesian A/B Testing Dashboard: From Theory to Production | Zubair Ashfaque</title>
    <meta name="description" content="A complete journey of building a production-ready Bayesian A/B testing framework with interactive Streamlit dashboard, exploring statistical rigor, PyMC, and real-world data analysis.">
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="alternate icon" href="../favicon.svg">

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <style>
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(to bottom, #0f172a, #1e293b);
            color: #e2e8f0;
        }

        .blog-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        .hero-gradient {
            background: linear-gradient(135deg, #06b6d4 0%, #3b82f6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .code-block {
            background: #1e293b;
            border-radius: 0.5rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            border: 1px solid #334155;
        }

        .concept-card {
            background: rgba(30, 41, 59, 0.5);
            border: 1px solid #334155;
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1rem 0;
            transition: all 0.3s ease;
        }

        .concept-card:hover {
            border-color: #06b6d4;
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(6, 182, 212, 0.2);
        }

        .github-card {
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
            border: 2px solid #06b6d4;
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .btn-primary {
            background: linear-gradient(135deg, #06b6d4 0%, #3b82f6 100%);
            color: white;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: all 0.3s ease;
            display: inline-block;
            text-decoration: none;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(6, 182, 212, 0.3);
        }

        .section-divider {
            height: 2px;
            background: linear-gradient(90deg, transparent, #06b6d4, transparent);
            margin: 3rem 0;
        }

        article h2 {
            font-size: 2rem;
            font-weight: 700;
            margin-top: 3rem;
            margin-bottom: 1rem;
            color: #06b6d4;
        }

        article h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: #3b82f6;
        }

        article p {
            line-height: 1.8;
            margin-bottom: 1rem;
            color: #cbd5e1;
        }

        article ul, article ol {
            margin: 1rem 0;
            padding-left: 2rem;
            color: #cbd5e1;
        }

        article li {
            margin: 0.5rem 0;
            line-height: 1.7;
        }

        .highlight-box {
            background: rgba(6, 182, 212, 0.1);
            border-left: 4px solid #06b6d4;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0.5rem;
        }

        .math-formula {
            background: #1e293b;
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: 'Courier New', monospace;
            color: #06b6d4;
            text-align: center;
            margin: 1rem 0;
            border: 1px solid #334155;
        }

        .tag {
            display: inline-block;
            background: rgba(6, 182, 212, 0.2);
            color: #06b6d4;
            padding: 0.25rem 0.75rem;
            border-radius: 1rem;
            font-size: 0.875rem;
            margin: 0.25rem;
            font-weight: 500;
        }

        .back-link {
            color: #06b6d4;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            margin-bottom: 2rem;
            transition: all 0.3s ease;
        }

        .back-link:hover {
            gap: 1rem;
            color: #3b82f6;
        }

        .stat-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .stat-box {
            background: rgba(6, 182, 212, 0.1);
            padding: 1.5rem;
            border-radius: 0.75rem;
            text-align: center;
            border: 1px solid #334155;
        }

        .stat-number {
            font-size: 2rem;
            font-weight: 700;
            color: #06b6d4;
        }

        .stat-label {
            font-size: 0.875rem;
            color: #94a3b8;
            margin-top: 0.5rem;
        }
    </style>
</head>
<body>
    <div class="blog-container">
        <!-- Back Navigation -->
        <a href="../index.html#journal" class="back-link">
            <i class="fas fa-arrow-left"></i>
            Back to Portfolio
        </a>

        <!-- Article Header -->
        <header class="mb-12">
            <div class="flex flex-wrap gap-2 mb-4">
                <span class="tag"><i class="fas fa-chart-bar mr-1"></i>Bayesian Statistics</span>
                <span class="tag"><i class="fas fa-code mr-1"></i>Python</span>
                <span class="tag"><i class="fas fa-flask mr-1"></i>Data Science</span>
                <span class="tag"><i class="fas fa-desktop mr-1"></i>Streamlit</span>
            </div>

            <h1 class="text-4xl md:text-5xl font-bold mb-4">
                Building a <span class="hero-gradient">Bayesian A/B Testing Dashboard</span>: From Theory to Production
            </h1>

            <div class="flex flex-wrap items-center gap-4 text-gray-400 text-sm mb-6">
                <span><i class="far fa-calendar mr-2"></i>October 27, 2025</span>
                <span><i class="far fa-clock mr-2"></i>12 min read</span>
                <span><i class="fas fa-tag mr-2"></i>Statistical Analysis</span>
            </div>

            <p class="text-xl text-gray-300 leading-relaxed">
                A comprehensive journey of building a production-ready Bayesian A/B testing framework with an interactive Streamlit dashboard. This article explores the intersection of statistical rigor, modern Python tooling, and practical data science - turning complex Bayesian inference into an accessible, visual experience.
            </p>
        </header>

        <!-- GitHub Card -->
        <div class="github-card">
            <div class="flex items-start gap-4">
                <div class="flex-shrink-0">
                    <i class="fab fa-github text-5xl text-cyan-400"></i>
                </div>
                <div class="flex-1">
                    <h3 class="text-xl font-bold text-white mb-2">
                        <i class="fas fa-code-branch mr-2 text-cyan-400"></i>
                        Open Source Project
                    </h3>
                    <p class="text-gray-300 mb-4">
                        Full source code with comprehensive documentation, unit tests (95%+ coverage), and example datasets. Built with modern Python best practices.
                    </p>
                    <div class="flex flex-wrap gap-3">
                        <a href="https://github.com/zubairashfaque/bayesian-ab-testing-dashboard" target="_blank" class="btn-primary">
                            <i class="fab fa-github mr-2"></i>View on GitHub
                        </a>
                        <a href="https://github.com/zubairashfaque/bayesian-ab-testing-dashboard#demo" target="_blank" class="btn-primary" style="background: linear-gradient(135deg, #8b5cf6 0%, #d946ef 100%);">
                            <i class="fas fa-play mr-2"></i>See Live Demo
                        </a>
                    </div>
                </div>
            </div>
        </div>

        <div class="section-divider"></div>

        <!-- Main Article -->
        <article>
            <h2><i class="fas fa-lightbulb mr-3"></i>The Motivation</h2>
            <p>
                A/B testing is the backbone of data-driven decision-making in tech. Yet most implementations rely on frequentist methods that provide binary answers: "statistically significant" or "not significant." But what if you could quantify <strong>exactly</strong> how much better variant B is than A? What if you could make decisions with nuanced confidence levels, not just p-values?
            </p>

            <p>
                That's where <strong>Bayesian A/B testing</strong> shines. It answers questions like:
            </p>

            <ul class="list-disc">
                <li>"What's the probability that variant B is at least 5% better than A?"</li>
                <li>"Should I stop the test now, or do I need more data?"</li>
                <li>"How confident am I that this difference is practically meaningful, not just statistically significant?"</li>
            </ul>

            <p>
                I built this dashboard to make Bayesian A/B testing accessible to anyone - from data scientists to product managers - with a beautiful, interactive interface that doesn't sacrifice statistical rigor.
            </p>

            <div class="highlight-box">
                <strong><i class="fas fa-info-circle mr-2"></i>Key Innovation:</strong>
                Combines rigorous Bayesian inference using PyMC with an intuitive Streamlit interface, making complex statistical concepts visual and actionable.
            </div>

            <div class="section-divider"></div>

            <h2><i class="fas fa-book-open mr-3"></i>ðŸ“– Technical Terms Explained</h2>

            <p>
                Before diving into the technical details, let's demystify the key terms and abbreviations you'll encounter. Think of this as your friendly dictionary - no PhD required!
            </p>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-vial mr-2"></i>A/B Testing
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A method of comparing two versions (A and B) of something to see which performs better.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Imagine you have two button colors on your website - blue and green. You show blue to 50% of users and green to the other 50%. After a week, you check which one got more clicks. That's A/B testing!
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> It's like flipping two coins to see which one lands on heads more often - except your "coins" are website features and "heads" is users taking action.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-brain mr-2"></i>Bayesian Statistics
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A statistical approach that updates beliefs based on new evidence. It answers "What's the probability of X being true?"
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> You think it rains 30% of days (your prior belief). Then you see dark clouds (new evidence). Now you think it's 80% likely to rain today (updated belief). That's Bayesian thinking!
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Like being a detective who starts with a hunch and refines it as more clues appear. Your certainty evolves with evidence.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-chart-area mr-2"></i>PyMC (Probabilistic Programming in Python)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Full Name:</strong> PyMC (formerly PyMC3, now version 5.25.1)
                </p>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A Python library for building Bayesian statistical models. It's the engine that does the mathematical heavy lifting.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Instead of writing hundreds of lines of math, you tell PyMC: "Here's my data, here's what I believe, now calculate the probabilities." PyMC handles the complex calculations automatically.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> PyMC is like having a genius math tutor who solves complicated probability problems for you while you focus on asking the right questions.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-random mr-2"></i>MCMC (Markov Chain Monte Carlo)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Abbreviation:</strong> MCMC = Markov Chain Monte Carlo
                </p>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A clever algorithm that explores complex probability distributions by taking smart random walks through possible values.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Imagine you're in a dark room searching for treasure. MCMC is like taking steps where each step depends on the last one, gradually moving toward areas with more treasure. After thousands of steps, you've mapped where the treasure is most likely located.
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Think of it as an intelligent drunk person's walk - they stumble randomly, but they're more likely to stumble toward interesting places. After many stumbles, the path reveals where they spent most time (the high-probability regions).
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Why we need it:</strong> Some probability calculations are mathematically impossible to solve directly. MCMC approximates the answer by simulating millions of scenarios.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-bolt mr-2"></i>NUTS Sampler (No-U-Turn Sampler)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Abbreviation:</strong> NUTS = No-U-Turn Sampler
                </p>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> An advanced MCMC algorithm that efficiently explores probability distributions. It's the "smart walker" method inside PyMC.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Instead of wandering aimlessly, NUTS is like a hiker with a compass and GPS. It takes bigger steps in promising directions and stops before going backwards (no U-turns!), making exploration 10x faster than basic MCMC.
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Imagine searching for gold in a mountain range. Basic MCMC is like walking randomly. NUTS is like having a metal detector that beeps louder near gold - you naturally move toward it and avoid backtracking.
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Why it matters:</strong> NUTS is the reason PyMC can analyze complex models in seconds instead of hours. It's the Ferrari of MCMC samplers.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-chart-pie mr-2"></i>Posterior Distribution
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> The updated probability distribution after seeing the data. It's your "final answer" showing what values are most likely.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Before testing, you guessed conversion rate was around 20% (prior). After seeing 100 visitors with 25 conversions, the posterior shows it's most likely 25%, but could reasonably be anywhere from 22% to 28%.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Like predicting tomorrow's weather. Your initial forecast (prior) gets updated with today's observations (data) to create tomorrow's prediction (posterior).
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-compress-arrows-alt mr-2"></i>HDI (Highest Density Interval)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Abbreviation:</strong> HDI = Highest Density Interval (also called Credible Interval)
                </p>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A range that contains the most probable values with a specified probability (usually 95%). It's Bayesian's version of a confidence interval.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> After your A/B test, the 95% HDI for conversion rate is [22%, 28%]. This means: "I'm 95% confident the true conversion rate is somewhere in this range, and values in this range are the most probable."
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Imagine you're guessing someone's age. Instead of saying "exactly 30," you say "95% sure they're between 28 and 32." The HDI is that age range.
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Key difference from frequentist CI:</strong> You can actually say "95% probability the value is in this range" - which is what everyone thinks confidence intervals mean (but they don't in frequentist stats!).
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-ruler-horizontal mr-2"></i>ROPE (Region of Practical Equivalence)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Abbreviation:</strong> ROPE = Region of Practical Equivalence
                </p>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A range of values you consider "too small to matter." It separates statistical significance from practical significance.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> You set ROPE to Â±1%. If variant B improves conversion rate by only 0.3%, it's "statistically different" but falls inside ROPE - meaning it's not worth implementing because the improvement is tiny.
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Like paying $1000 to gain $10. Sure, you "gain money" (statistically significant), but it's a terrible business decision (not practically significant). ROPE prevents you from celebrating meaningless wins.
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Why it's powerful:</strong> Forces you to define "How much improvement actually matters to my business?" before looking at results. Prevents chasing tiny, worthless gains.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-dna mr-2"></i>Beta Distribution
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A probability distribution that models percentages/proportions. It's bounded between 0 and 1, making it perfect for conversion rates, click-through rates, etc.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Beta(3, 7) represents: "I saw 3 successes and 7 failures, so I think the true rate is around 30% (3 Ã· 10), but I'm not super confident yet because my sample is small."
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Like a rubber band stretched between 0% and 100%. The parameters (Î± and Î²) determine where it bulges most - that's your best guess for the true percentage.
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Why it's used:</strong> Beta distributions have a magical property - when you multiply them with binomial data (success/failure), the math works out perfectly. No complex calculations needed!
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-layer-group mr-2"></i>Beta-Binomial Model
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> The combination of Beta prior + Binomial likelihood = Beta posterior. It's the foundation of Bayesian A/B testing for conversion rates.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong><br>
                    â€¢ <strong>Prior:</strong> Beta(1, 1) = "I think conversion rate is somewhere between 0-100%, no strong belief"<br>
                    â€¢ <strong>Data:</strong> 20 conversions out of 100 visitors<br>
                    â€¢ <strong>Posterior:</strong> Beta(1+20, 1+80) = Beta(21, 81) = "Now I think it's around 20%, with some uncertainty"
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Like updating a weather forecast. Your morning prediction (prior) + today's observations (data) = tonight's updated forecast (posterior). The Beta-Binomial model is the formula that does this update automatically.
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Why it's elegant:</strong> This is called a "conjugate prior" - it means the posterior has the same form as the prior (both Beta). You can update your beliefs infinitely as new data arrives, and the math always stays simple.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-check-double mr-2"></i>Convergence Diagnostics
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> Tools to check if your MCMC sampler has explored the probability space enough to give reliable answers.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Key Metrics:</strong><br>
                    â€¢ <strong>R-hat (RÌ‚):</strong> Should be < 1.01. Measures if multiple chains agree. Think of it as "Do 4 different hikers all find the same treasure location?"<br>
                    â€¢ <strong>ESS (Effective Sample Size):</strong> Should be > 400. Measures how many "independent" samples you have. Like checking if your survey respondents are actually different people, not the same person filling it out 1000 times.
                </p>
                <p class="text-gray-400 text-sm mb-3">
                    <strong>Analogy:</strong> Like checking if your coffee is fully brewed. Convergence diagnostics tell you: "Yes, the mixing is complete, your results are reliable" or "No, keep brewing, it's not ready yet."
                </p>
                <p class="text-gray-400 text-sm italic">
                    <strong>Why it matters:</strong> MCMC can give you wrong answers if you stop it too early. These diagnostics are your safety check, like a smoke detector for bad statistics.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-chart-line mr-2"></i>ArviZ (Exploratory Analysis of Bayesian Models)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Full Name:</strong> ArviZ (pronounced "arviz")
                </p>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A Python library for analyzing and visualizing Bayesian models. It calculates HDI, creates trace plots, computes diagnostics, and makes beautiful visualizations.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> After PyMC runs your model, ArviZ creates the graphs showing your posterior distributions, calculates the 95% HDI, checks R-hat values, and generates trace plots - all with one line of code.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> If PyMC is the race car that runs the calculations, ArviZ is the dashboard that shows you speed, fuel level, and GPS location in easy-to-read gauges.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-expand-arrows-alt mr-2"></i>Prior Distribution (Prior)
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> Your initial belief about a parameter before seeing any data. It encodes what you know (or don't know) before running the experiment.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong><br>
                    â€¢ <strong>Uninformative:</strong> Beta(1, 1) = "I have no idea what the conversion rate is"<br>
                    â€¢ <strong>Weakly Informative:</strong> Beta(3, 7) = "I think it's around 30%, but I'm open to the data changing my mind"<br>
                    â€¢ <strong>Informative:</strong> Beta(30, 70) = "Based on 5 years of history, I'm pretty sure it's 30% Â± 5%"
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Like entering a courtroom. The prior is your opinion before hearing witnesses. Some people come in neutral (uninformative), others have a hunch (weakly informative), others are already 90% convinced (informative). The data is the testimony that may or may not change your mind.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-percentage mr-2"></i>Conjugate Prior
                </h4>
                <p class="text-gray-300 mb-2">
                    <strong>Definition:</strong> A prior that, when combined with a specific likelihood, produces a posterior of the same family. It's a mathematical convenience that makes calculations instant.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> Beta prior + Binomial likelihood = Beta posterior. No MCMC needed! The formula is: Beta(Î±, Î²) + [k successes, n trials] = Beta(Î±+k, Î²+n-k).
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Like LEGO blocks that click together perfectly. Conjugate priors are "compatible" with certain data types, making the math fit together seamlessly without glue (MCMC).
                </p>
            </div>

            <div class="section-divider"></div>

            <h2><i class="fas fa-cogs mr-3"></i>Technical Architecture</h2>

            <h3>The Stack</h3>
            <div class="stat-grid">
                <div class="stat-box">
                    <i class="fab fa-python text-3xl text-cyan-400 mb-2"></i>
                    <div class="stat-label">Python 3.9+</div>
                </div>
                <div class="stat-box">
                    <i class="fas fa-chart-line text-3xl text-blue-400 mb-2"></i>
                    <div class="stat-label">PyMC 5.25.1</div>
                </div>
                <div class="stat-box">
                    <i class="fas fa-desktop text-3xl text-purple-400 mb-2"></i>
                    <div class="stat-label">Streamlit 1.50</div>
                </div>
                <div class="stat-box">
                    <i class="fas fa-database text-3xl text-green-400 mb-2"></i>
                    <div class="stat-label">Pandas + NumPy</div>
                </div>
            </div>

            <p>
                The dashboard is built on a solid foundation of modern Python libraries:
            </p>

            <ul class="list-disc">
                <li><strong>PyMC 5</strong>: Bayesian modeling with NUTS sampler for MCMC inference</li>
                <li><strong>ArviZ</strong>: Posterior analysis, HDI calculation, and convergence diagnostics</li>
                <li><strong>Streamlit</strong>: Multi-page reactive dashboard with auto-discovery</li>
                <li><strong>Plotly</strong>: Interactive visualizations for posterior distributions and effect sizes</li>
                <li><strong>SciPy</strong>: Statistical functions for Beta distributions and priors</li>
            </ul>

            <h3>Project Structure</h3>
            <div class="code-block">
                <pre class="text-sm text-gray-300"><code>bayesian-ab-dashboard/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bayesian_ab_test.py    # Core Beta-Binomial model
â”‚   â”‚   â””â”€â”€ priors.py               # Prior library (6 predefined)
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ effect_size.py          # Lift, relative risk, odds ratio
â”‚   â”‚   â”œâ”€â”€ decision_maker.py       # ROPE-based decisions
â”‚   â”‚   â””â”€â”€ convergence.py          # R-hat, ESS diagnostics
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py               # Cookie Cats dataset
â”‚   â”‚   â”œâ”€â”€ validator.py            # Data quality checks
â”‚   â”‚   â””â”€â”€ preprocessor.py         # Feature engineering
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ plotting.py             # Matplotlib + Seaborn helpers
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py                      # Main landing page
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ 1_ðŸ“Š_Data_Explorer.py
â”‚       â”œâ”€â”€ 2_ðŸŽ¯_Bayesian_Analysis.py
â”‚       â”œâ”€â”€ 3_ðŸ“š_Prior_Library.py
â”‚       â””â”€â”€ 4_ðŸ“–_Learn_Bayesian.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                       # 46 unit tests (95%+ coverage)
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ cookie_cats_analysis.py     # Real data demo
â””â”€â”€ pyproject.toml                  # Poetry dependencies</code></pre>
            </div>

            <div class="section-divider"></div>

            <h2><i class="fas fa-brain mr-3"></i>The Mathematical Foundation</h2>

            <p>
                At the heart of this dashboard is the <strong>Beta-Binomial conjugate model</strong> - the elegant mathematical relationship that makes Bayesian A/B testing tractable.
            </p>

            <h3>The Beta-Binomial Model</h3>
            <p>
                For conversion rate estimation, we use the Beta distribution as our prior because it's the <em>conjugate prior</em> for the Binomial likelihood:
            </p>

            <div class="math-formula">
                Prior: Î¸ ~ Beta(Î±, Î²)<br>
                Likelihood: data | Î¸ ~ Binomial(n, Î¸)<br>
                Posterior: Î¸ | data ~ Beta(Î± + conversions, Î² + non-conversions)
            </div>

            <p>
                This conjugacy means the posterior has a closed-form solution - we don't need MCMC for simple cases. However, I still use PyMC's NUTS sampler because:
            </p>

            <ol class="list-decimal">
                <li><strong>Scalability</strong>: The framework easily extends to hierarchical models</li>
                <li><strong>Convergence diagnostics</strong>: ArviZ provides R-hat, ESS, and trace plots</li>
                <li><strong>Flexibility</strong>: Users can experiment with non-conjugate priors</li>
                <li><strong>Education</strong>: It demonstrates real-world MCMC workflows</li>
            </ol>

            <h3>Prior Library Design</h3>
            <p>
                One of the most critical (and often overlooked) aspects of Bayesian analysis is <strong>prior selection</strong>. I designed a library of 6 carefully calibrated priors:
            </p>

            <div class="concept-card">
                <h4 class="text-lg font-semibold text-cyan-400 mb-2">
                    <i class="fas fa-balance-scale mr-2"></i>Uninformative Prior - Beta(1, 1)
                </h4>
                <p class="text-sm text-gray-300">
                    Uniform distribution. Use when you have zero domain knowledge and want the data to speak for itself. Equivalent to a frequentist maximum likelihood estimate.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-lg font-semibold text-cyan-400 mb-2">
                    <i class="fas fa-leaf mr-2"></i>Weakly Informative (Default) - Beta(3, 7)
                </h4>
                <p class="text-sm text-gray-300">
                    Centered at 30% conversion rate with weak confidence (n=10 effective observations). The <strong>recommended default</strong> for most A/B tests. Provides gentle regularization without overwhelming the data.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-lg font-semibold text-cyan-400 mb-2">
                    <i class="fas fa-graduation-cap mr-2"></i>Informative - Customizable
                </h4>
                <p class="text-sm text-gray-300">
                    Specify your expected rate (Î¼) and confidence level (50-95%). Automatically calculates Î± and Î² using method of moments. Perfect when you have historical data.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-lg font-semibold text-cyan-400 mb-2">
                    <i class="fas fa-shield-alt mr-2"></i>Skeptical - Beta(16, 24)
                </h4>
                <p class="text-sm text-gray-300">
                    Centered at control rate (40%) with moderate confidence. Assumes treatment is similar to control - requires strong evidence to declare a winner. Great for incrementa improvements.
                </p>
            </div>

            <p>
                The other priors (Pessimistic, Optimistic) allow users to encode different expectations about performance. This flexibility is crucial - <strong>your prior should reflect your actual beliefs, not just mathematical convenience</strong>.
            </p>

            <div class="section-divider"></div>

            <h2><i class="fas fa-chart-area mr-3"></i>The Dashboard Experience</h2>

            <p>
                The dashboard consists of 4 interconnected pages, each designed with a specific purpose:
            </p>

            <h3>1. Data Explorer - Understanding Your Data</h3>
            <p>
                Before running any analysis, you need to <em>know</em> your data. This page provides:
            </p>
            <ul class="list-disc">
                <li><strong>Automated validation</strong>: Checks for missing values, duplicates, invalid ranges</li>
                <li><strong>Distribution analysis</strong>: Histograms and box plots of game rounds by variant</li>
                <li><strong>Retention comparison</strong>: 1-day vs 7-day retention rates side-by-side</li>
                <li><strong>Engagement segmentation</strong>: User buckets (low/medium/high/very high) with retention correlation</li>
                <li><strong>Preprocessing preview</strong>: See exactly how data is cleaned and split</li>
            </ul>

            <div class="highlight-box">
                <strong><i class="fas fa-database mr-2"></i>Real Data:</strong>
                Uses the Cookie Cats dataset (90K+ users) from Kaggle - a mobile game A/B test comparing gate placement at level 30 vs 40.
            </div>

            <h3>2. Bayesian Analysis - The Core Engine</h3>
            <p>
                This is where the magic happens. The page is organized into:
            </p>

            <p><strong>Configuration Sidebar:</strong></p>
            <ul class="list-disc">
                <li>Prior selection (dropdown with 6 options + custom)</li>
                <li>ROPE width (Â±0.1% to Â±5%)</li>
                <li>MCMC parameters (500-4000 draws, 2-4 chains)</li>
                <li>Credible interval level (80%-99%)</li>
            </ul>

            <p><strong>Results Tabs:</strong></p>
            <ol class="list-decimal">
                <li><strong>Posterior Distributions</strong>: Side-by-side KDE plots with HDI shading, mean/median lines, and overlap visualization</li>
                <li><strong>Effect Size</strong>: Lift distribution, probability B > A, relative risk, odds ratio with confidence bands</li>
                <li><strong>Summary Report</strong>: Decision recommendation (Adopt/Reject/Inconclusive), probabilities in each ROPE region, statistical power</li>
                <li><strong>Detailed Statistics</strong>: Full posterior summaries, convergence diagnostics (R-hat, ESS), trace plots, and sample data</li>
            </ol>

            <div class="code-block">
                <pre class="text-sm text-gray-300"><code># Core inference code (simplified)
def fit(self, control_conversions, control_trials,
        treatment_conversions, treatment_trials):
    with pm.Model() as model:
        # Priors
        theta_A = pm.Beta("theta_A", alpha=self.prior_alpha,
                          beta=self.prior_beta)
        theta_B = pm.Beta("theta_B", alpha=self.prior_alpha,
                          beta=self.prior_beta)

        # Likelihoods
        obs_A = pm.Binomial("obs_A", n=control_trials,
                            p=theta_A, observed=control_conversions)
        obs_B = pm.Binomial("obs_B", n=treatment_trials,
                            p=theta_B, observed=treatment_conversions)

        # Derived quantities
        delta = pm.Deterministic("delta", theta_B - theta_A)
        lift = pm.Deterministic("lift", (theta_B - theta_A) / theta_A)

        # Sample with NUTS
        trace = pm.sample(draws=2000, tune=1000, chains=4,
                          return_inferencedata=True)

    return trace</code></pre>
            </div>

            <h3>3. Prior Library - Interactive Exploration</h3>
            <p>
                Most people don't have intuition for Beta distributions. This page fixes that:
            </p>
            <ul class="list-disc">
                <li><strong>Compare priors</strong>: Select multiple priors and overlay their PDFs</li>
                <li><strong>Custom builder</strong>: Sliders for Î± and Î² with real-time updates to mean, std, mode, and 95% CI</li>
                <li><strong>Detailed descriptions</strong>: For each prior, see use cases, characteristics, and when to apply</li>
                <li><strong>Quick stats</strong>: Metrics displayed as cards for easy comparison</li>
            </ul>

            <p>
                The interactive Î±/Î² sliders are particularly powerful for <strong>building intuition</strong>:
            </p>
            <ul class="list-disc">
                <li>Small Î±, Î² (< 2): Weak belief, data-driven</li>
                <li>Medium Î±, Î² (2-20): Moderate influence on posterior</li>
                <li>Large Î±, Î² (> 20): Strong belief, need lots of data to change</li>
            </ul>

            <h3>4. Learn Bayesian - Educational Content</h3>
            <p>
                The dashboard isn't just a tool - it's a <strong>learning platform</strong>. This page features:
            </p>
            <ul class="list-disc">
                <li><strong>Why Bayesian?</strong>: Side-by-side comparison with frequentist methods</li>
                <li><strong>Key Concepts</strong>: Interactive modules explaining Prior, Likelihood, Posterior, HDI, ROPE, MCMC</li>
                <li><strong>Mathematical Foundations</strong>: Beta distribution properties, Bayes' theorem, conjugacy</li>
                <li><strong>Interpretation Guide</strong>: Step-by-step walkthrough of reading results</li>
                <li><strong>Best Practices</strong>: When to use which prior, how to set ROPE, stopping rules</li>
            </ul>

            <div class="section-divider"></div>

            <h2><i class="fas fa-images mr-3"></i>ðŸ“¸ Dashboard Walkthrough</h2>

            <p>
                The following screenshots showcase the complete Bayesian A/B Testing Dashboard workflow, from initial data exploration through final analysis and decision making. Each image captures a critical step in turning raw data into actionable insights.
            </p>

            <h3>Welcome Page</h3>
            <p>
                The landing page provides an overview of the complete Bayesian analysis workflow and highlights key advantages over traditional frequentist methods.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/dashboard_home.png" alt="Dashboard Home" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The homepage introduces users to the five-step workflow: Upload Data, Configure Analysis, Run Analysis, View Results, and Make Decisions. It emphasizes the intuitive interpretation of results with direct probability statements rather than confusing p-values.
            </p>

            <h3>Data Explorer Page</h3>
            <p>
                Before running any analysis, the Data Explorer page helps you understand your dataset through comprehensive visualizations and quality checks.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/data_explorer.png" alt="Data Explorer Overview" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> Dataset overview showing 90,189 total users split between control (gate_30: 44,700 users) and treatment (gate_40: 45,489 users) groups. The interface displays sample data with columns for user ID, variant assignment, game rounds played, and retention metrics.
            </p>

            <h4 class="text-lg font-semibold text-blue-400 mt-6 mb-3">Distribution Analysis Tab</h4>
            <p>
                The first tab provides deep insights into how game rounds are distributed across the entire dataset and by experimental variant.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/exploratory_distribution_analysis.png" alt="Distribution Analysis" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Distribution Analysis tab reveals the distribution of game rounds with two complementary views. The left histogram shows that the overall distribution is heavily right-skewed with most users playing fewer than 10 rounds, while the right panel displays box plots comparing game rounds by variant. Summary statistics at the bottom reveal that gate_30 users averaged 52.46 rounds with a standard deviation of 256.72, while gate_40 users averaged 51.29 rounds with a standard deviation of 103.29. Both variants show similar median values (5 rounds) but gate_30 exhibits higher variability with a maximum of 49,854 rounds compared to gate_40's maximum of 2,640 rounds.
            </p>

            <h4 class="text-lg font-semibold text-blue-400 mt-6 mb-3">Retention Comparison Tab</h4>
            <p>
                The second tab focuses specifically on retention metrics, comparing both 1-day and 7-day retention rates between experimental groups.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/exploratory_retention_comparison.png" alt="Retention Comparison" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Retention Comparison tab presents side-by-side bar charts showing retention rates for both variants. For gate_30, the 1-day retention is 44.82% and 7-day retention is 19.02% (based on 44,700 users). For gate_40, the 1-day retention is 44.23% and 7-day retention is 18.20% (based on 45,489 users). The visual comparison makes it immediately clear that while both variants have similar 1-day retention, the control group (gate_30) maintains a slightly higher 7-day retention rate, which is the primary metric of interest for the A/B test.
            </p>

            <h4 class="text-lg font-semibold text-blue-400 mt-6 mb-3">Engagement Analysis Tab</h4>
            <p>
                The third tab segments users by engagement level to understand how player behavior relates to retention outcomes.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/exploratory_engagement_analysis.png" alt="Engagement Analysis" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Engagement Analysis tab categorizes users into four engagement levels based on game rounds played: Low (0-20 rounds), Medium (20-50 rounds), Very High (100+ rounds), and High (50-100 rounds). The pie chart on the left shows that 53.1% of users fall into the Low engagement category, 20.4% are Medium, 12.1% are High, and 14.4% are Very High. The bar chart on the right reveals a critical insight: 7-day retention increases dramatically with engagement level, rising from around 10% for Low engagement users to approximately 90% for Very High engagement users. This strong positive correlation suggests that strategies to increase early engagement could substantially improve long-term retention.
            </p>

            <h4 class="text-lg font-semibold text-blue-400 mt-6 mb-3">Data Quality Tab</h4>
            <p>
                The fourth tab performs comprehensive data validation to ensure the dataset is clean and ready for statistical analysis.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/exploratory_data_quality.png" alt="Data Quality" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Data Quality Check tab confirms that the dataset passes all validation requirements with a green success message: "Data validation passed - no issues found!" The dashboard shows zero missing values, zero duplicate user IDs, and confirms all data types are valid. This rigorous quality check provides confidence that the subsequent Bayesian analysis will be based on clean, reliable data. The expandable "Column Information" section allows users to examine detailed metadata about each variable in the dataset.
            </p>

            <h3>Bayesian Analysis Page</h3>
            <p>
                The core analysis page provides complete control over prior selection, MCMC parameters, and decision thresholds.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/bayesian_analysis.png" alt="Bayesian Analysis Configuration" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> Analysis configuration interface showing the Cookie Cats experiment results: Control group (35,760 users, 19.24% retention) vs Treatment group (36,391 users, 18.29% retention). The sidebar allows selection of prior distributions, ROPE bounds, MCMC sampling parameters, and decision thresholds.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/prior_selection_6.png" alt="Prior Selection Options" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Prior Type dropdown menu offers six carefully crafted prior distributions: Uninformative (Flat), Weakly Informative, Mobile Game Default, Informative, Skeptical, Pessimistic, and Optimistic. Each prior encodes different levels of prior knowledge and assumptions about expected retention rates.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/posterior_distributions.png" alt="Posterior Distributions" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> Posterior distribution plots for control and treatment groups show overlapping probability distributions. The control group (p_control) has a mean of 0.19 (19% retention) while the treatment group (p_treatment) has a mean of 0.18 (18% retention). The 95% Highest Density Interval (HDI) provides the range of plausible values for each group's true retention rate.
            </p>

            <h3>Effect Size Analysis</h3>
            <p>
                Understanding the difference between groups is crucial for making informed decisions.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/effect_size_full_view.png" alt="Effect Size Distribution" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The effect size distribution provides a complete picture of the difference between treatment and control groups. The distribution is centered at a mean of negative 0.0095 (shown by the solid green vertical line), indicating the treatment group has slightly lower retention. The 95% HDI ranges from negative 0.0152 to negative 0.0036, displayed at the top of the chart. The shaded light blue region represents the ROPE (Region of Practical Equivalence), which captures differences considered too small to matter practically. A critical insight emerges from the statistics displayed: only 0.04% of the distribution falls above zero (meaning treatment is better than control), while a full 57% of the distribution falls within the ROPE. The red dashed vertical line at zero represents no effect. This visualization elegantly combines the direction of effect (negative), magnitude (small), statistical confidence (95% HDI), and practical significance (ROPE), giving decision-makers all the information they need in a single, interpretable chart.
            </p>

            <h3>Results Summary</h3>
            <p>
                The comprehensive summary report combines all key visualizations and statistics in a publication-ready format.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/summary_report_complete.png" alt="Complete Summary Report" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Bayesian A/B Test Summary Report presents a holistic view that synthesizes all analysis results into one comprehensive visualization. The top panel displays the posterior distributions for both control (blue) and treatment (red) groups with dashed vertical lines marking the observed retention rates at 19.24% and 18.29% respectively. The solid colored histograms represent our updated beliefs after incorporating the data with our prior knowledge. The bottom-left panel shows the effect size distribution as a green histogram centered at negative 0.0095, with the red dashed line at zero clearly visible to the right of most of the distribution mass. The bottom-right panel contains a fascinating probability plot showing a horizontal line at approximately 0.04, representing the vanishingly small probability that treatment performs better than control. The comprehensive metrics table at the bottom provides exact numbers: control has an observed rate of 0.1924 and posterior mean of 0.1926 with 95% HDI of [0.1884, 0.1966], while treatment shows observed rate of 0.1829 and posterior mean of 0.1831 with 95% HDI of [0.1791, 0.1872]. The difference column confirms the mean effect of negative 0.0095 with credible interval [negative 0.0152, negative 0.0036].
            </p>

            <h3>Detailed Statistics and Decision Recommendation</h3>
            <p>
                Beyond visualizations, the dashboard provides comprehensive numerical statistics and actionable business recommendations.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/detailed_statistics_full.png" alt="Detailed Statistics" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Detailed Statistics tab organizes all numerical results in a clear, scannable table format. For the control group (gate_30), the posterior mean is 0.1926 with a standard deviation of 0.0021, and the 95% HDI spans from 0.1884 to 0.1966. The treatment group (gate_40) shows a posterior mean of 0.1831 with standard deviation of 0.0021 and 95% HDI of [0.1791, 0.1872]. The Effect Size section reveals a mean difference of negative 0.0095 with standard deviation of 0.0029 and 95% HDI ranging from negative 0.0152 to negative 0.0036. The Relative Uplift section translates these absolute differences into percentage terms, showing a mean relative change of negative 4.91% with 95% HDI spanning from negative 7.77% to negative 1.90%. These statistics provide stakeholders with precise numerical values for presentations, reports, and decision-making documentation.
            </p>

            <div style="margin: 2rem 0; text-align: center;">
                <img src="images/decision_recommendation.png" alt="Decision Recommendation" style="width: 100%; max-width: 900px; border-radius: 0.5rem; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); border: 1px solid #334155;">
            </div>

            <p class="text-gray-400 text-sm italic" style="margin-top: 1rem;">
                <strong>Caption:</strong> The Decision section delivers the actionable bottom line: "CONTROL BETTER" with a clear recommendation stating "Control is better with 100.0% probability. Expected drop: negative 0.95%". This removes all ambiguity and translates complex statistical analysis into straightforward business guidance. The Prior Used section maintains complete transparency by documenting exactly which prior distribution informed the analysis: a Skeptical prior with parameters Î¼ equals 0.48, alpha equals 16.0, beta equals 24.0, mean equals 0.400, and standard deviation equals 0.077. This documentation ensures reproducibility and allows other analysts to understand and critique the assumptions underlying the analysis. The yellow tip at the bottom encourages experimentation: "Try different priors and see how they affect the results. Bayesian analysis naturally incorporates prior beliefs!" This educational element reminds users that one of Bayesian methods' greatest strengths is the ability to formally incorporate domain knowledge through prior selection.
            </p>

            <div class="highlight-box">
                <strong><i class="fas fa-lightbulb mr-2"></i>Key Insights from Analysis:</strong>
                <p class="mt-2">
                    The Bayesian analysis of the Cookie Cats experiment reveals a nuanced story that goes beyond simple "statistically significant" or "not significant" labels. The posterior distributions show that while there is overlap between the control and treatment groups, the data provides strong evidence that the control group (gate at level 30) performs better than the treatment group (gate at level 40) in terms of 7-day retention.
                </p>
                <p class="mt-2">
                    The effect size analysis is particularly illuminating. With a mean difference of negative 0.95 percentage points and a 95% HDI that ranges entirely below zero (from negative 1.52 to negative 0.36 percentage points), we can be highly confident that moving the gate from level 30 to level 40 actually decreases player retention. The probability that treatment is better than control is vanishingly small at just 0.04%, which translates to overwhelming evidence favoring the control variant.
                </p>
                <p class="mt-2">
                    However, the practical significance is where the analysis becomes truly interesting. A full 57% of the effect size distribution falls within the ROPE (Region of Practical Equivalence), suggesting that while the difference is real, it may not be large enough to warrant immediate action. This is the beauty of Bayesian analysis: rather than forcing a binary decision, it provides the full probability distribution that captures both the direction and magnitude of effects, along with our uncertainty about them.
                </p>
                <p class="mt-2">
                    The decision recommendation is clear: "CONTROL BETTER" with 100.0% probability. This means that based on the data and our prior beliefs, we should maintain the gate at level 30 rather than moving it to level 40. The expected negative impact of negative 0.95 percentage points may seem small, but at scale with millions of players, this translates to thousands of additional retained players by keeping the gate at the earlier level.
                </p>
            </div>

            <div class="section-divider"></div>

            <h2><i class="fas fa-wrench mr-3"></i>Technical Challenges & Solutions</h2>

            <h3>Challenge 1: MCMC Convergence</h3>
            <p>
                <strong>Problem</strong>: With weak priors and small sample sizes, MCMC chains can fail to converge.
            </p>
            <p>
                <strong>Solution</strong>: Implemented comprehensive diagnostics:
            </p>
            <ul class="list-disc">
                <li>R-hat < 1.01 check (automatic warning if exceeded)</li>
                <li>ESS > 400 per chain validation</li>
                <li>Trace plots for visual inspection</li>
                <li>Auto-adjust tuning samples based on data size</li>
            </ul>

            <h3>Challenge 2: ROPE Interpretation</h3>
            <p>
                <strong>Problem</strong>: Users confused about ROPE width and decision thresholds.
            </p>
            <p>
                <strong>Solution</strong>: Created visual ROPE explorer:
            </p>
            <ul class="list-disc">
                <li>Slider with real-time effect on decision boundaries</li>
                <li>Probability bars showing % in ROPE, below, and above</li>
                <li>Context-specific defaults (Â±1% for conversion rate tests)</li>
                <li>Explanation tooltips at every step</li>
            </ul>

            <h3>Challenge 3: Performance with Large Datasets</h3>
            <p>
                <strong>Problem</strong>: 90K+ rows caused slow PyMC inference and Streamlit re-runs.
            </p>
            <p>
                <strong>Solution</strong>: Multi-layered optimization:
            </p>
            <ul class="list-disc">
                <li><code>@st.cache_data</code> for data loading and preprocessing</li>
                <li>Sufficient statistics: Only pass (conversions, trials) to PyMC, not raw data</li>
                <li>Incremental sampling: Start with 500 draws for preview, option to increase</li>
                <li>JAX backend for PyMC (30% faster sampling)</li>
            </ul>

            <div class="section-divider"></div>

            <h2><i class="fas fa-rocket mr-3"></i>Impact & Results</h2>

            <div class="stat-grid">
                <div class="stat-box">
                    <div class="stat-number">46</div>
                    <div class="stat-label">Unit Tests</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">95%+</div>
                    <div class="stat-label">Code Coverage</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">6</div>
                    <div class="stat-label">Prior Options</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number">90K+</div>
                    <div class="stat-label">Real Data Rows</div>
                </div>
            </div>

            <p>
                The Cookie Cats analysis revealed a <strong>fascinating result</strong>:
            </p>

            <div class="highlight-box">
                <strong><i class="fas fa-chart-line mr-2"></i>Finding:</strong>
                Moving the gate from level 30 to level 40 led to a <strong>-1.2% to -0.5% decrease</strong> in 7-day retention (95% HDI). With 96.8% probability that treatment is worse than control, the decision was clear: <strong>keep the gate at level 30</strong>.
            </div>

            <p>
                This demonstrates the power of Bayesian analysis - we're not just saying "statistically significant" - we're quantifying the <em>magnitude</em> and <em>direction</em> of the effect with uncertainty intervals.
            </p>

            <h2><i class="fas fa-graduation-cap mr-3"></i>Key Learnings</h2>

            <ol class="list-decimal">
                <li>
                    <strong>Prior selection matters</strong>: The same data with different priors can lead to different decisions - especially with small samples. Always document your prior choice and run sensitivity analyses.
                </li>
                <li>
                    <strong>ROPE > P-values</strong>: Defining a Region of Practical Equivalence forces you to think about <em>business impact</em>, not just statistical significance. A 0.1% lift might be "significant" but worthless to implement.
                </li>
                <li>
                    <strong>Visualization is critical</strong>: Posterior distributions, HDI intervals, and trace plots make complex inference interpretable. Never just report a number - show the full distribution.
                </li>
                <li>
                    <strong>Education scales impact</strong>: Building the "Learn Bayesian" page took extra time, but it multiplies the dashboard's value. Now stakeholders can <em>understand</em> the results, not just trust them.
                </li>
                <li>
                    <strong>Testing is non-negotiable</strong>: With 46 unit tests covering edge cases (zero conversions, identical variants, invalid inputs), I caught 12 bugs before they hit production. Statistical tools require rigor.
                </li>
            </ol>

            <div class="section-divider"></div>

            <h2><i class="fas fa-code-branch mr-3"></i>Future Enhancements</h2>

            <p>
                This project is a starting point. Here's what's next:
            </p>

            <ul class="list-disc">
                <li><strong>Hierarchical models</strong>: Multi-variant testing, segment-level inference</li>
                <li><strong>Sequential testing</strong>: Implement early stopping rules with Bayes factors</li>
                <li><strong>Covariate adjustment</strong>: Logistic regression with Bayesian inference</li>
                <li><strong>Time-series analysis</strong>: Day-over-day retention curves with credible bands</li>
                <li><strong>API endpoint</strong>: FastAPI service for programmatic access</li>
                <li><strong>Docker deployment</strong>: One-command setup for production environments</li>
            </ul>

            <div class="section-divider"></div>

            <h2><i class="fas fa-heart mr-3"></i>Final Thoughts</h2>

            <p>
                Building this dashboard taught me that <strong>great data science tools aren't just technically sound - they're accessible, visual, and educational</strong>. Bayesian A/B testing is more powerful than frequentist methods, but it's often avoided because it seems complex.
            </p>

            <p>
                By combining PyMC's statistical rigor with Streamlit's interactive design, I've created something that doesn't sacrifice mathematical correctness for usability. It does both.
            </p>

            <p>
                If you're working on A/B testing, experimentation platforms, or just want to learn Bayesian inference - I hope this project inspires you. The code is open source, the tests are comprehensive, and the documentation is extensive.
            </p>

            <div class="highlight-box">
                <strong><i class="fas fa-code mr-2"></i>Try it yourself:</strong>
                Clone the repo, download the Cookie Cats data, and run <code>streamlit run streamlit_app/app.py</code>. You'll be analyzing Bayesian A/B tests in 60 seconds.
            </div>

            <p class="mt-8 text-center text-gray-400">
                <i class="fas fa-chart-line mr-2"></i>
                Built with Python, PyMC, Streamlit, and a passion for statistical clarity.
            </p>
        </article>

        <!-- GitHub Card Bottom -->
        <div class="github-card mt-12">
            <h3 class="text-2xl font-bold text-white mb-4 text-center">
                <i class="fab fa-github mr-3 text-cyan-400"></i>
                Explore the Full Project
            </h3>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-4 text-center">
                <div>
                    <i class="fas fa-code text-3xl text-cyan-400 mb-2"></i>
                    <p class="text-gray-300 text-sm">2,000+ lines of production-ready Python</p>
                </div>
                <div>
                    <i class="fas fa-check-circle text-3xl text-green-400 mb-2"></i>
                    <p class="text-gray-300 text-sm">46 unit tests with 95%+ coverage</p>
                </div>
                <div>
                    <i class="fas fa-book text-3xl text-purple-400 mb-2"></i>
                    <p class="text-gray-300 text-sm">Comprehensive documentation & examples</p>
                </div>
            </div>
            <div class="text-center mt-6">
                <a href="https://github.com/zubairashfaque/bayesian-ab-testing-dashboard" target="_blank" class="btn-primary text-lg">
                    <i class="fab fa-github mr-2"></i>View Repository on GitHub
                </a>
            </div>
        </div>

        <div class="section-divider"></div>

        <!-- Author Section -->
        <div class="bg-slate-800 rounded-lg p-6 mt-12">
            <div class="flex items-center gap-4">
                <div class="w-16 h-16 rounded-full bg-gradient-to-r from-cyan-400 to-blue-500 flex items-center justify-center text-white text-2xl font-bold">
                    ZA
                </div>
                <div>
                    <h4 class="text-lg font-bold text-white">Zubair Ashfaque</h4>
                    <p class="text-gray-400 text-sm">AI Tech Lead | Data Scientist | Bayesian Enthusiast</p>
                </div>
            </div>
            <p class="text-gray-300 mt-4">
                Building AI systems that transform data into insights. Passionate about statistical rigor, open source, and making complex methods accessible.
            </p>
            <div class="flex gap-4 mt-4">
                <a href="https://github.com/zubairashfaque" target="_blank" class="text-cyan-400 hover:text-cyan-300">
                    <i class="fab fa-github text-xl"></i>
                </a>
                <a href="https://linkedin.com/in/zubairashfaque" target="_blank" class="text-cyan-400 hover:text-cyan-300">
                    <i class="fab fa-linkedin text-xl"></i>
                </a>
                <a href="mailto:mianashfaque@gmail.com" class="text-cyan-400 hover:text-cyan-300">
                    <i class="fas fa-envelope text-xl"></i>
                </a>
            </div>
        </div>

        <!-- Back to Top -->
        <div class="text-center mt-12">
            <a href="../index.html#journal" class="back-link text-lg">
                <i class="fas fa-arrow-left"></i>
                Back to Portfolio
            </a>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
