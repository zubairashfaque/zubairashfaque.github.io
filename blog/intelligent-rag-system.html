<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building Production-Ready RAG Systems: From Documents to AI Answers | Zubair Ashfaque</title>
    <meta name="description" content="A comprehensive guide to building an intelligent RAG system that transforms documents into an AI-powered knowledge base. Learn the complete pipeline from document processing to answer generation.">
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="assets/blog-styles.css">
    <style>
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(to bottom, #0f172a, #1e293b);
            color: #e2e8f0;
        }

        .blog-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        .hero-gradient {
            background: linear-gradient(135deg, #8b5cf6 0%, #d946ef 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .code-block {
            background: #1e293b;
            border-radius: 0.5rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            border: 1px solid #334155;
            position: relative;
        }

        .concept-card {
            background: rgba(30, 41, 59, 0.5);
            border: 1px solid #334155;
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1rem 0;
            transition: all 0.3s ease;
        }

        .concept-card:hover {
            border-color: #8b5cf6;
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(139, 92, 246, 0.2);
        }

        .analogy-card {
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
            border: 2px solid #06b6d4;
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .blueprint-box {
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
            border: 2px solid #8b5cf6;
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .btn-primary {
            background: linear-gradient(135deg, #06b6d4 0%, #3b82f6 100%);
            color: white;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: all 0.3s ease;
            display: inline-block;
            text-decoration: none;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(6, 182, 212, 0.3);
        }

        .section-divider {
            height: 2px;
            background: linear-gradient(90deg, transparent, #8b5cf6, transparent);
            margin: 3rem 0;
        }

        article h2 {
            font-size: 2rem;
            font-weight: 700;
            margin-top: 3rem;
            margin-bottom: 1rem;
            color: #06b6d4;
        }

        article h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: #8b5cf6;
        }

        article p {
            font-size: 1.125rem;
            line-height: 1.8;
            margin-bottom: 1.25rem;
            color: #cbd5e1;
        }

        article ul, article ol {
            font-size: 1.125rem;
            line-height: 1.8;
            margin-bottom: 1.25rem;
            padding-left: 1.5rem;
            color: #cbd5e1;
        }

        article li {
            margin-bottom: 0.5rem;
        }

        .breadcrumb {
            display: flex;
            gap: 0.5rem;
            align-items: center;
            margin-bottom: 2rem;
            font-size: 0.875rem;
            color: #94a3b8;
        }

        .breadcrumb a {
            color: #06b6d4;
            text-decoration: none;
            transition: color 0.3s;
        }

        .breadcrumb a:hover {
            color: #8b5cf6;
        }

        .copy-btn {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: #334155;
            color: #e2e8f0;
            border: none;
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            cursor: pointer;
            font-size: 0.875rem;
            transition: all 0.3s;
        }

        .copy-btn:hover {
            background: #475569;
        }

        @media (max-width: 768px) {
            .blog-container {
                padding: 1rem;
            }

            article h2 {
                font-size: 1.5rem;
            }

            article p, article ul, article ol {
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav class="bg-slate-900/50 backdrop-blur-md border-b border-slate-700 sticky top-0 z-50">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16">
                <a href="../index.html" class="text-xl font-bold bg-gradient-to-r from-cyan-400 to-blue-500 bg-clip-text text-transparent">
                    Zubair Ashfaque
                </a>
                <div class="flex gap-6">
                    <a href="../index.html#journal" class="text-slate-300 hover:text-cyan-400 transition">
                        <i class="fas fa-arrow-left mr-2"></i>Back to Journal
                    </a>
                </div>
            </div>
        </div>
    </nav>

    <div class="blog-container">
        <!-- Breadcrumb -->
        <div class="breadcrumb">
            <a href="../index.html">Home</a>
            <i class="fas fa-chevron-right text-xs"></i>
            <a href="../index.html#journal">Journal</a>
            <i class="fas fa-chevron-right text-xs"></i>
            <span>Building Production-Ready RAG Systems</span>
        </div>

        <!-- Hero Section -->
        <header class="mb-12">
            <h1 class="text-5xl font-bold mb-4 hero-gradient">
                Building Production-Ready RAG Systems: From Documents to AI Answers
            </h1>
            <p class="text-xl text-slate-400 mb-6">
                A Complete Blueprint for Transforming Documents into an Intelligent Knowledge Base
            </p>
            <div class="flex flex-wrap gap-4 text-sm text-slate-400">
                <span><i class="far fa-calendar mr-2"></i>November 4, 2025</span>
                <span><i class="far fa-clock mr-2"></i>8 min read</span>
                <span><i class="far fa-user mr-2"></i>Zubair Ashfaque</span>
            </div>
            <div class="flex flex-wrap gap-2 mt-4">
                <span class="px-3 py-1 bg-violet-500/20 text-violet-300 rounded-full text-sm">RAG</span>
                <span class="px-3 py-1 bg-violet-500/20 text-violet-300 rounded-full text-sm">LLM</span>
                <span class="px-3 py-1 bg-violet-500/20 text-violet-300 rounded-full text-sm">Vector Databases</span>
                <span class="px-3 py-1 bg-violet-500/20 text-violet-300 rounded-full text-sm">Production AI</span>
                <span class="px-3 py-1 bg-violet-500/20 text-violet-300 rounded-full text-sm">ChromaDB</span>
            </div>
        </header>

        <div class="section-divider"></div>

        <!-- Main Article Content -->
        <article>
            <h2><i class="fas fa-lightbulb mr-3"></i>The Motivation</h2>

            <p>
                Document intelligence is transforming how organizations access and leverage knowledge. But here's the reality most teams face: you have hundreds of PDFs, Word documents, research papers, and internal documentation scattered across folders and cloud storage. Finding specific information feels like searching for a needle in a haystack. You know the answer exists "somewhere in those documents," but locating it manually takes hours or even days.
            </p>

            <p>
                Traditional keyword search falls short because it only finds exact matches. If you search for "machine learning" but the document says "ML" or "artificial intelligence," you miss it entirely. Context and synonyms are invisible to Ctrl+F. Even when search engines find fragments, you lose the surrounding context. Was this a recommendation? A warning? Part of a larger argument? Fragments without context are often useless or misleading.
            </p>

            <p>
                This matters because knowledge workers spend <strong>30-40% of their time</strong> searching for information they know exists. That's 12-16 hours per week spent hunting instead of producing value. Organizational knowledge remains trapped in scattered documents, and new employees spend months learning what's "buried in that old presentation from 2019." The questions this article answers are:
            </p>

            <ul class="list-disc">
                <li>"How do I build a RAG system that actually understands document context?"</li>
                <li>"What's the complete pipeline from document upload to AI-generated answers?"</li>
                <li>"How do I optimize for cost while maintaining accuracy?"</li>
                <li>"What architecture patterns make RAG systems production-ready?"</li>
            </ul>

            <p>
                I built this guide to provide a systematic blueprint for creating an intelligent RAG system that processes multiple document formats, chunks them intelligently, generates semantic embeddings, and retrieves relevant context for AI-powered answer generation. You'll see the complete production-grade architecture that I've implemented and deployed.
            </p>

            <div class="highlight-box" style="background: rgba(139, 92, 246, 0.1); border-left: 4px solid #8b5cf6; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 0.5rem;">
                <strong><i class="fas fa-info-circle mr-2"></i>Key Innovation:</strong>
                This system combines YAML-driven configuration, dual-database architecture (ChromaDB + SQLite), permanent embedding caching, and two-stage retrieval to create a flexible, cost-efficient RAG platform that scales from personal projects to enterprise knowledge bases.
            </div>

            <div class="section-divider"></div>

            <h2><i class="fas fa-bullseye mr-3"></i>The Challenge</h2>

            <p>
                The <strong>challenge</strong> is that building a RAG system that actually works in production requires solving multiple interconnected problems simultaneously. You cannot just upload documents to a vector database and expect useful results. The naive approach fails at every stage: documents get mangled during extraction, chunks break mid-sentence losing context, embeddings waste money on duplicate content, and retrieval returns irrelevant results.
            </p>

            <p>
                Most developers hit these frustration points: PDF text extraction produces garbled output with broken formatting. Document chunking splits paragraphs arbitrarily, cutting sentences in half and destroying semantic coherence. Embedding costs spiral out of control because every re-upload regenerates the same vectors. Vector search returns documents that match keywords but miss the actual meaning of the query. And when you finally get results, you cannot trace them back to their source documents because metadata gets lost in the pipeline.
            </p>

            <p>
                The root cause is that RAG is not a single component but an entire pipeline where each stage depends on the previous one. Poor chunking leads to poor embeddings. Poor embeddings lead to poor retrieval. Poor retrieval leads to poor answers. You need to get every stage right, and you need them to work together as a coherent system. This is the challenge we will solve systematically in this article.
            </p>

            <div class="section-divider"></div>

            <h2><i class="fas fa-search mr-3"></i>Lucifying the Problem</h2>

            <p>
                Let's <strong>lucify</strong> this concept using an analogy from everyday life that captures the essence of how RAG systems work.
            </p>

            <div class="analogy-card">
                <p>
                    Imagine you are a researcher who needs to write a comprehensive report about climate change. You have access to a university library containing ten thousand books, journals, and papers on the topic. You cannot possibly read all ten thousand documents, so you work with a brilliant research librarian who knows the collection intimately.
                </p>

                <p>
                    Here's how the process works: You ask the librarian a specific question: "What are the primary causes of rising sea levels?" The librarian does not just hand you a random stack of books. Instead, they perform a sophisticated multi-step process. First, they quickly scan through the catalog system to identify fifty books and papers that might contain relevant information based on subject tags and abstracts. This is fast but imprecise.
                </p>

                <p>
                    Then comes the crucial second step: the librarian actually opens those fifty candidates and reads the relevant chapters, comparing them carefully against your specific question. They narrow it down to the top five sources that most precisely address your question. But they do not stop there. The librarian then reads those five sources, extracts the key passages, and writes you a concise summary that synthesizes the main points while citing exactly which books and page numbers each point came from.
                </p>

                <p>
                    You receive a one-page summary that answers your question comprehensively, backed by citations to five authoritative sources. If you need more detail, you know exactly which books to pull and which pages to read. The librarian has transformed ten thousand documents into actionable knowledge in minutes.
                </p>
            </div>

            <p>
                This analogy maps directly to RAG systems. Your document collection is the library. Your question is the user query. The catalog system is the vector database that performs fast semantic search. The librarian's careful reading of fifty candidates is the cross-encoder reranking stage that ensures precision. The final summary is the AI-generated answer with source citations. Just like the librarian, a RAG system finds relevant information, verifies its relevance, and synthesizes it into a useful answer.
            </p>

            <p>
                The analogy breaks down in one important way: the librarian works with ten thousand documents over years of training, while RAG systems can process millions of documents and adapt to new content instantly. This computational scalability is precisely why RAG systems are transforming knowledge work. The principles remain the same, but the scale and speed are unprecedented.
            </p>

            <div class="section-divider"></div>

            <h2><i class="fas fa-book-open mr-3"></i>Lucifying the Tech Terms</h2>

            <p>
                To solve this problem effectively, we first need to <strong>lucify</strong> the key technical terms that appear throughout RAG system development. Understanding these terms clearly will make the blueprint implementation much easier to follow.
            </p>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-robot mr-2"></i>RAG (Retrieval-Augmented Generation)
                </h4>
                <p class="text-base text-gray-300 mb-2">
                    <strong>Definition:</strong> An AI architecture that combines information retrieval with language model generation. Instead of relying solely on what the language model memorized during training, RAG systems first retrieve relevant documents from an external knowledge base, then use those documents as context for generating the final answer. This prevents hallucinations and keeps answers grounded in real source material.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> You ask a chatbot: "What were our Q3 sales figures?" Without RAG, the chatbot can only answer based on outdated training data. With RAG, the chatbot first searches your company's financial documents, retrieves the Q3 report, then generates an answer based on that specific document. The answer is current, accurate, and includes citations to the source.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Think of RAG like a student taking an open-book exam rather than a closed-book exam. In a closed-book exam, the student relies entirely on memorized information which may be incomplete or forgotten. In an open-book exam, the student can reference textbooks and notes to provide more accurate, detailed, and current answers. RAG systems are the open-book approach to AI.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-vector-square mr-2"></i>Vector Embeddings
                </h4>
                <p class="text-base text-gray-300 mb-2">
                    <strong>Definition:</strong> Numerical representations of text that capture semantic meaning in a format computers can mathematically compare. When you convert a sentence into a vector embedding, you transform it into a list of 1,536 or 3,072 numbers that encode its meaning, not just its keywords. Similar meanings produce similar vectors, even if the exact words differ.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> The sentence "The cat sat on the mat" might become a vector like [0.23, -0.45, 0.67, ..., 0.12] with 1,536 numbers. A similar sentence "A feline rested on the rug" would have a similar vector like [0.25, -0.43, 0.65, ..., 0.14]. Even though they use completely different words, their vectors are mathematically close because their meanings are similar.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Think of vector embeddings like GPS coordinates for meaning. Just as GPS converts a physical location into latitude and longitude numbers that can be compared to calculate distances, embeddings convert text meaning into numerical coordinates in a high-dimensional space. Texts with similar meanings end up close together, while texts with different meanings end up far apart.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-cut mr-2"></i>Chunking Strategy
                </h4>
                <p class="text-base text-gray-300 mb-2">
                    <strong>Definition:</strong> The process of splitting large documents into smaller, semantically coherent segments that preserve context and meaning. Documents are too long to embed or process as single units, so we split them into chunks of 500-2,000 characters. Good chunking preserves paragraph boundaries, avoids breaking sentences mid-thought, and includes overlap so information does not fall between chunks.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> A 50-page research paper gets split into 100 chunks of roughly 1,000 characters each. Instead of cutting arbitrarily at character 1,000, the chunker respects paragraph boundaries and sentence structure. Chunk 1 might contain the introduction, chunk 2 the methodology, and each chunk overlaps by 200 characters to ensure no context is lost at boundaries.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Think of chunking like dividing a textbook into chapters and sections. You would not cut a textbook every 50 pages regardless of content. You divide it at natural boundaries where topics change. Similarly, intelligent chunking respects the document's natural structure, ensuring each chunk is a coherent unit of meaning.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-database mr-2"></i>Dual-Database Architecture
                </h4>
                <p class="text-base text-gray-300 mb-2">
                    <strong>Definition:</strong> A system design that uses two specialized databases working together: ChromaDB (a vector database) for semantic similarity search and SQLite (a relational database) for structured metadata and analytics. ChromaDB excels at finding similar content but lacks relational querying capabilities. SQLite excels at structured queries but cannot perform vector search. Together, they provide complete functionality.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> When you query "machine learning benefits," ChromaDB quickly finds the 50 most semantically similar document chunks by comparing vector embeddings. Meanwhile, SQLite tracks which documents were processed when, how much embedding generation cost, which queries users run most frequently, and what retrieval strategies perform best. Each database does what it does best.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Think of it like a library using both a card catalog and a checkout system. The card catalog helps you find books by topic (ChromaDB finds content by meaning), while the checkout system tracks who borrowed what and when (SQLite tracks metadata and analytics). Neither system could replace the other, but together they make the library fully functional.
                </p>
            </div>

            <div class="concept-card">
                <h4 class="text-xl font-semibold text-cyan-400 mb-3">
                    <i class="fas fa-search mr-2"></i>Semantic Search
                </h4>
                <p class="text-base text-gray-300 mb-2">
                    <strong>Definition:</strong> A search approach that finds documents based on meaning and context rather than exact keyword matches. Semantic search uses vector embeddings to understand what you are looking for and retrieves documents that match the intent of your query, even if they use completely different terminology. This is fundamentally different from traditional keyword search.
                </p>
                <p class="text-gray-400 text-sm mb-2">
                    <strong>Simple Example:</strong> You search for "cost-effective laptop options." Traditional keyword search would only find documents containing those exact words. Semantic search finds documents about "budget-friendly computers," "affordable notebooks," "inexpensive portable PCs," and "economical computing solutions" because it recognizes these phrases have the same meaning as your query.
                </p>
                <p class="text-gray-400 text-sm">
                    <strong>Analogy:</strong> Think of semantic search like asking a knowledgeable person for help versus using a phone book. A phone book only works if you know the exact name you are looking for. But a knowledgeable person understands what you mean and can suggest relevant results even if you phrase your question differently. Semantic search is the knowledgeable person approach.
                </p>
            </div>

            <div class="section-divider"></div>

            <h2><i class="fas fa-drafting-compass mr-3"></i>Making the Blueprint</h2>

            <p>
                Now, let's <strong>make the blueprint</strong> for implementing a production-ready RAG system. This systematic approach takes you from raw documents to AI-generated answers with source citations.
            </p>

            <h3><i class="fas fa-clipboard-list mr-2"></i>The Six-Stage RAG Pipeline Blueprint</h3>

            <div class="blueprint-box">
                <p class="text-lg font-semibold text-violet-300 mb-4">Overview: Building an End-to-End Document Intelligence Platform</p>

                <div class="space-y-4">
                    <div>
                        <p class="font-semibold text-cyan-400">Stage One: Document Ingestion and Text Extraction</p>
                        <p>
                            The first stage handles multiple document formats and extracts clean text while preserving structure. We support PDF, DOCX, TXT, HTML, and Markdown files. The system automatically detects file encoding, handles corrupted files gracefully, and preserves important metadata like filenames, page numbers, and positions. We use specialized libraries for each format: PyPDF2 for PDFs, python-docx for Word documents, and BeautifulSoup for HTML. The output is clean, structured text with metadata that threads through the entire pipeline.
                        </p>
                    </div>

                    <div>
                        <p class="font-semibold text-cyan-400">Stage Two: Intelligent Chunking with Context Preservation</p>
                        <p>
                            Documents get split into semantically coherent chunks that respect natural boundaries. We use recursive character splitting that tries to break at paragraphs first, then sentences, then words, and finally characters if necessary. Chunk sizes range from 500 to 2,000 characters depending on the use case, with 200-character overlap to ensure no information falls between chunks. The chunking strategy is YAML-configurable, allowing you to experiment with different approaches without code changes. Each chunk preserves metadata from its source document.
                        </p>
                    </div>

                    <div>
                        <p class="font-semibold text-cyan-400">Stage Three: Embedding Generation with Permanent Caching</p>
                        <p>
                            Each chunk gets converted into a 1,536 or 3,072-dimensional vector using OpenAI's embedding models. But here's the key innovation: we use content-hash based deduplication. Before calling the API, we compute a SHA-256 hash of the chunk text and check if we have already generated an embedding for that exact content. If yes, we retrieve the cached embedding for free. If no, we call the API once and cache the result permanently in SQLite. This means the first upload of a document costs money, but every subsequent upload is free. Cache hit rates typically reach 85-95% on real workloads.
                        </p>
                    </div>

                    <div>
                        <p class="font-semibold text-cyan-400">Stage Four: Dual-Database Storage Architecture</p>
                        <p>
                            Embeddings get stored in ChromaDB for fast vector similarity search, while metadata gets stored in SQLite for analytics and tracking. ChromaDB provides persistent on-disk storage with automatic indexing and can search 10,000 documents in under 100 milliseconds using cosine similarity. SQLite tracks ingestion jobs with timestamps and costs, records document metadata, and logs query history with latencies and results. The dual-database approach provides both fast semantic search and comprehensive analytics.
                        </p>
                    </div>

                    <div>
                        <p class="font-semibold text-cyan-400">Stage Five: Two-Stage Retrieval Pipeline</p>
                        <p>
                            Retrieval happens in two stages for optimal accuracy and speed. Stage one performs fast vector search using cosine similarity to retrieve the top 50 candidate chunks in 50-100 milliseconds. Stage two applies cross-encoder reranking using sentence-transformers models that process the query and each candidate together to produce more accurate relevance scores. This takes 200-300 milliseconds but dramatically improves precision. The reranked results get sorted and the top 10 chunks proceed to answer generation. This two-stage approach combines the speed of vector search with the accuracy of cross-encoders.
                        </p>
                    </div>

                    <div>
                        <p class="font-semibold text-cyan-400">Stage Six: AI-Powered Answer Generation with Citations</p>
                        <p>
                            The retrieved chunks become context for GPT-3.5 or GPT-4 to generate a natural language answer. The system constructs a prompt that includes the user's question and the top 10 retrieved chunks, instructing the model to synthesize an answer while citing specific sources. The response includes the AI-generated answer, confidence scoring (High/Medium/Low), and exact citations showing which documents and chunks contributed to each part of the answer. This prevents hallucinations because the model only works with actual retrieved content, and users can verify answers by checking the cited sources.
                        </p>
                    </div>
                </div>
            </div>

            <h3><i class="fas fa-project-diagram mr-2"></i>Visual Flow: The Complete RAG Pipeline</h3>

            <div class="bg-gradient-to-br from-slate-800 to-slate-900 rounded-2xl p-8 my-8 border-4 border-cyan-500/50 shadow-2xl">
                <div class="text-center mb-8">
                    <h4 class="text-2xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-cyan-400 to-blue-400 mb-3">
                        üîÑ Production RAG System Pipeline
                    </h4>
                    <p class="text-slate-400">
                        From document upload to AI answer in six systematic stages
                    </p>
                </div>

                <!-- Row 1: Upload + Load + Chunk -->
                <div class="flex flex-col md:flex-row items-center justify-center gap-4 mb-6">
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-violet-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-violet-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">üì§</span>
                                </div>
                                <span class="text-violet-300 font-semibold">Upload Docs</span>
                            </div>
                            <p class="text-sm text-slate-400">PDF, DOCX, TXT, HTML, MD</p>
                        </div>
                    </div>
                    <div class="text-violet-400 text-2xl">‚Üí</div>
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-cyan-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-cyan-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">üìÑ</span>
                                </div>
                                <span class="text-cyan-300 font-semibold">Extract Text</span>
                            </div>
                            <p class="text-sm text-slate-400">Clean text + metadata</p>
                        </div>
                    </div>
                    <div class="text-cyan-400 text-2xl">‚Üí</div>
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-blue-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-blue-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">‚úÇÔ∏è</span>
                                </div>
                                <span class="text-blue-300 font-semibold">Chunk Text</span>
                            </div>
                            <p class="text-sm text-slate-400">1000 chars, 200 overlap</p>
                        </div>
                    </div>
                </div>

                <div class="flex justify-center mb-6">
                    <div class="text-blue-400 text-3xl">‚Üì</div>
                </div>

                <!-- Row 2: Embed + Cache + Store -->
                <div class="flex flex-col md:flex-row items-center justify-center gap-4 mb-6">
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-indigo-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-indigo-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">ü§ñ</span>
                                </div>
                                <span class="text-indigo-300 font-semibold">Generate Embeddings</span>
                            </div>
                            <p class="text-sm text-slate-400">1536-dim vectors (OpenAI)</p>
                        </div>
                    </div>
                    <div class="text-indigo-400 text-2xl">‚Üí</div>
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-fuchsia-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-fuchsia-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">üíæ</span>
                                </div>
                                <span class="text-fuchsia-300 font-semibold">Check Cache</span>
                            </div>
                            <p class="text-sm text-slate-400">SHA-256 deduplication</p>
                        </div>
                    </div>
                    <div class="text-fuchsia-400 text-2xl">‚Üí</div>
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-pink-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-pink-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">üóÑÔ∏è</span>
                                </div>
                                <span class="text-pink-300 font-semibold">Store Dual-DB</span>
                            </div>
                            <p class="text-sm text-slate-400">ChromaDB + SQLite</p>
                        </div>
                    </div>
                </div>

                <div class="flex justify-center mb-6">
                    <div class="text-pink-400 text-3xl">‚Üì</div>
                </div>

                <!-- Row 3: Query + Retrieve + Generate -->
                <div class="flex flex-col md:flex-row items-center justify-center gap-4 mb-6">
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-rose-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-rose-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">‚ùì</span>
                                </div>
                                <span class="text-rose-300 font-semibold">User Query</span>
                            </div>
                            <p class="text-sm text-slate-400">Natural language question</p>
                        </div>
                    </div>
                    <div class="text-rose-400 text-2xl">‚Üí</div>
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-orange-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-orange-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">üîç</span>
                                </div>
                                <span class="text-orange-300 font-semibold">Two-Stage Retrieval</span>
                            </div>
                            <p class="text-sm text-slate-400">Vector search + reranking</p>
                        </div>
                    </div>
                    <div class="text-orange-400 text-2xl">‚Üí</div>
                    <div class="flex-1 max-w-xs">
                        <div class="bg-slate-700/50 border-2 border-amber-500 rounded-xl p-4 hover:scale-105 transition-transform">
                            <div class="flex items-center gap-3 mb-2">
                                <div class="w-12 h-12 bg-amber-500/20 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">ü§ñ</span>
                                </div>
                                <span class="text-amber-300 font-semibold">Generate Answer</span>
                            </div>
                            <p class="text-sm text-slate-400">GPT-4 + citations</p>
                        </div>
                    </div>
                </div>

                <div class="flex justify-center mb-6">
                    <div class="text-amber-400 text-3xl">‚Üì</div>
                </div>

                <!-- Row 4: Final Answer -->
                <div class="flex justify-center">
                    <div class="max-w-2xl w-full">
                        <div class="bg-gradient-to-br from-green-500/20 to-emerald-500/20 border-3 border-green-500 rounded-xl p-6">
                            <div class="flex items-center gap-3 mb-3">
                                <div class="w-12 h-12 bg-green-500/30 rounded-lg flex items-center justify-center">
                                    <span class="text-2xl">‚úÖ</span>
                                </div>
                                <span class="text-green-300 font-semibold text-lg">AI Answer with Source Citations</span>
                            </div>
                            <p class="text-slate-300">Comprehensive answer backed by verified sources</p>
                            <p class="text-sm text-slate-400 mt-2">Confidence score + document references + cost tracking</p>
                        </div>
                    </div>
                </div>
            </div>

            <p>
                This blueprint provides a clear roadmap for implementing a production-ready RAG system. Each stage builds on the previous one, and following this sequence ensures you handle edge cases, optimize costs, and maintain quality throughout the pipeline. The architecture is modular, allowing you to swap components or adjust configurations without rebuilding the entire system.
            </p>

            <div class="section-divider"></div>

            <h2><i class="fas fa-cogs mr-3"></i>Executing the Blueprint</h2>

            <p>
                <strong>Let's carry out the blueprint plan.</strong> I have implemented this complete RAG system with production-ready code. The full implementation with configuration files, Streamlit dashboard, and deployment instructions is available on my GitHub repository at <a href="https://github.com/zubairashfaque/intelligent-rag-system" class="text-cyan-400 hover:text-cyan-300 underline">github.com/zubairashfaque/intelligent-rag-system</a>.
            </p>

            <h3><i class="fas fa-file-code mr-2"></i>Stage One & Two: Document Ingestion and Chunking</h3>

            <div class="code-block">
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="far fa-copy mr-1"></i>Copy
                </button>
                <pre><code class="language-python">from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import hashlib
from pathlib import Path

def load_document(file_path: str, source_filename: str):
    """
    Load document with format detection and metadata preservation.

    Args:
        file_path: Path to the document
        source_filename: Original filename for metadata tracking

    Returns:
        List of LangChain documents with metadata
    """
    file_ext = Path(file_path).suffix.lower()

    # Select appropriate loader based on file extension
    loaders = {
        '.pdf': PyPDFLoader,
        '.docx': Docx2txtLoader,
        '.txt': TextLoader,
    }

    if file_ext not in loaders:
        raise ValueError(f"Unsupported file type: {file_ext}")

    # Load document with appropriate loader
    loader = loaders[file_ext](file_path)
    documents = loader.load()

    # Add original filename to metadata for citation tracking
    for doc in documents:
        doc.metadata['original_filename'] = source_filename
        doc.metadata['file_type'] = file_ext

    return documents

def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):
    """
    Split documents into chunks while preserving context.

    Uses recursive character splitting: tries to break at paragraphs first,
    then sentences, then words, then characters.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""],  # Hierarchical splitting
        length_function=len,
    )

    chunks = text_splitter.split_documents(documents)

    # Add chunk-specific metadata
    for i, chunk in enumerate(chunks):
        chunk.metadata['chunk_id'] = i
        chunk.metadata['chunk_size'] = len(chunk.page_content)

    return chunks

# Example usage
file_path = "uploads/research_paper.pdf"
original_name = "Machine_Learning_Basics.pdf"

# Load and chunk the document
documents = load_document(file_path, original_name)
chunks = chunk_documents(documents, chunk_size=1000, chunk_overlap=200)

print(f"Loaded {len(documents)} pages")
print(f"Created {len(chunks)} chunks")
print(f"First chunk: {chunks[0].page_content[:100]}...")
print(f"Metadata: {chunks[0].metadata}")</code></pre>
            </div>

            <p>
                This code handles document loading with automatic format detection and creates semantically coherent chunks. The recursive splitter respects document structure by trying to break at natural boundaries. Crucially, we preserve the original filename in metadata so that when users see an answer, they know exactly which document it came from.
            </p>

            <h3><i class="fas fa-brain mr-2"></i>Stage Three: Embedding Generation with Caching</h3>

            <div class="code-block">
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="far fa-copy mr-1"></i>Copy
                </button>
                <pre><code class="language-python">import openai
import hashlib
import sqlite3
import pickle
import numpy as np

class EmbeddingCache:
    """
    Permanent cache for embeddings using content-hash deduplication.
    """

    def __init__(self, cache_db_path="data/embedding_cache.db"):
        self.cache_db_path = cache_db_path
        self._init_cache_db()

    def _init_cache_db(self):
        """Create cache table if it doesn't exist."""
        conn = sqlite3.connect(self.cache_db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS embedding_cache (
                content_hash TEXT PRIMARY KEY,
                embedding BLOB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()

    def get_hash(self, text: str) -> str:
        """Generate SHA-256 hash of text content."""
        return hashlib.sha256(text.encode()).hexdigest()

    def get(self, content_hash: str):
        """Retrieve cached embedding if it exists."""
        conn = sqlite3.connect(self.cache_db_path)
        cursor = conn.cursor()
        cursor.execute(
            "SELECT embedding FROM embedding_cache WHERE content_hash = ?",
            (content_hash,)
        )
        result = cursor.fetchone()
        conn.close()

        if result:
            return pickle.loads(result[0])
        return None

    def store(self, content_hash: str, embedding):
        """Store embedding in cache."""
        conn = sqlite3.connect(self.cache_db_path)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT OR REPLACE INTO embedding_cache (content_hash, embedding) VALUES (?, ?)",
            (content_hash, pickle.dumps(embedding))
        )
        conn.commit()
        conn.close()

def generate_embeddings_with_cache(chunks, model="text-embedding-3-small"):
    """
    Generate embeddings for chunks with permanent caching.

    Returns:
        List of embeddings and cache statistics
    """
    cache = EmbeddingCache()
    embeddings = []
    cache_hits = 0
    api_calls = 0

    for chunk in chunks:
        # Compute content hash
        content_hash = cache.get_hash(chunk.page_content)

        # Check cache first
        cached_embedding = cache.get(content_hash)

        if cached_embedding is not None:
            embeddings.append(cached_embedding)
            cache_hits += 1
        else:
            # Cache miss - call OpenAI API
            response = openai.embeddings.create(
                input=chunk.page_content,
                model=model
            )
            embedding = response.data[0].embedding

            # Store in cache for future use
            cache.store(content_hash, embedding)
            embeddings.append(embedding)
            api_calls += 1

    stats = {
        'total_chunks': len(chunks),
        'cache_hits': cache_hits,
        'api_calls': api_calls,
        'cache_hit_rate': cache_hits / len(chunks) if chunks else 0,
        'estimated_savings': cache_hits * 0.0001  # Rough estimate
    }

    return embeddings, stats

# Example usage
embeddings, stats = generate_embeddings_with_cache(chunks)

print(f"Cache Statistics:")
print(f"  Total chunks: {stats['total_chunks']}")
print(f"  Cache hits: {stats['cache_hits']}")
print(f"  API calls: {stats['api_calls']}")
print(f"  Cache hit rate: {stats['cache_hit_rate']:.1%}")
print(f"  Estimated savings: ${stats['estimated_savings']:.4f}")</code></pre>
            </div>

            <p>
                The embedding cache uses SHA-256 content hashing to detect duplicate chunks across all documents. The first time you process a chunk, you pay the API cost. Every subsequent time that exact content appears (even in a different document), you retrieve the embedding from cache for free. This typically saves 85-95% of embedding costs in production.
            </p>

            <h3><i class="fas fa-database mr-2"></i>Stage Four & Five: Dual Storage and Two-Stage Retrieval</h3>

            <div class="code-block">
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="far fa-copy mr-1"></i>Copy
                </button>
                <pre><code class="language-python">import chromadb
from sentence_transformers import CrossEncoder
import numpy as np

class RAGRetriever:
    """
    Two-stage retrieval with ChromaDB vector search and cross-encoder reranking.
    """

    def __init__(self, collection_name="documents"):
        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(path="data/chroma_db")
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # Use cosine similarity
        )

        # Initialize cross-encoder for reranking (CPU-optimized)
        self.reranker = CrossEncoder(
            'cross-encoder/ms-marco-MiniLM-L-6-v2',
            device='cpu'  # Prevents GPU errors, minimal performance impact
        )

    def add_documents(self, chunks, embeddings):
        """Store chunks and embeddings in ChromaDB."""
        ids = [f"chunk_{i}" for i in range(len(chunks))]
        documents = [chunk.page_content for chunk in chunks]
        metadatas = [chunk.metadata for chunk in chunks]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )

        print(f"Stored {len(chunks)} chunks in ChromaDB")

    def retrieve(self, query, top_k_initial=50, top_k_final=10):
        """
        Two-stage retrieval: fast vector search + precise reranking.

        Args:
            query: User's question
            top_k_initial: Number of candidates from vector search
            top_k_final: Number of results after reranking

        Returns:
            List of top reranked results with scores
        """
        # Generate query embedding
        response = openai.embeddings.create(
            input=query,
            model="text-embedding-3-small"
        )
        query_embedding = response.data[0].embedding

        # Stage 1: Fast vector search (50-100ms)
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k_initial,
            include=['documents', 'metadatas', 'distances']
        )

        # Extract candidates
        candidates = []
        for i in range(len(results['ids'][0])):
            candidates.append({
                'id': results['ids'][0][i],
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'vector_score': 1 - results['distances'][0][i]  # Convert distance to similarity
            })

        # Stage 2: Cross-encoder reranking (200-300ms)
        if len(candidates) > 0:
            pairs = [(query, cand['text']) for cand in candidates]
            rerank_scores = self.reranker.predict(pairs)

            # Add rerank scores and sort
            for i, cand in enumerate(candidates):
                cand['rerank_score'] = float(rerank_scores[i])

            candidates.sort(key=lambda x: x['rerank_score'], reverse=True)

        # Return top N after reranking
        return candidates[:top_k_final]

# Example usage
retriever = RAGRetriever(collection_name="my_documents")

# Store documents
retriever.add_documents(chunks, embeddings)

# Query the system
query = "What are the main benefits of machine learning?"
results = retriever.retrieve(query, top_k_initial=50, top_k_final=10)

print(f"\nTop Results:")
for i, result in enumerate(results[:3], 1):
    print(f"\n{i}. Score: {result['rerank_score']:.4f}")
    print(f"   Source: {result['metadata']['original_filename']}")
    print(f"   Text: {result['text'][:150]}...")</code></pre>
            </div>

            <p>
                The two-stage retrieval combines speed with accuracy. Vector search is extremely fast but sometimes returns false positives. Cross-encoder reranking is slower but catches subtle semantic relationships. By applying the expensive reranking only to the top 50 candidates instead of all documents, we get high accuracy at reasonable computational cost.
            </p>

            <h3><i class="fas fa-robot mr-2"></i>Stage Six: AI Answer Generation with Citations</h3>

            <div class="code-block">
                <button class="copy-btn" onclick="copyCode(this)">
                    <i class="far fa-copy mr-1"></i>Copy
                </button>
                <pre><code class="language-python">import openai

def generate_answer_with_citations(query, retrieved_chunks, model="gpt-3.5-turbo"):
    """
    Generate AI answer from retrieved context with source citations.

    Args:
        query: User's question
        retrieved_chunks: List of retrieved document chunks
        model: OpenAI model to use

    Returns:
        Dictionary with answer, citations, and confidence score
    """
    # Build context from retrieved chunks
    context_parts = []
    for i, chunk in enumerate(retrieved_chunks, 1):
        source = chunk['metadata']['original_filename']
        text = chunk['text']
        context_parts.append(f"[Source {i}: {source}]\n{text}\n")

    context = "\n".join(context_parts)

    # Construct prompt with explicit citation instructions
    prompt = f"""You are a helpful research assistant. Answer the user's question based ONLY on the provided context.

IMPORTANT RULES:
1. Only use information from the provided sources
2. Cite your sources using [Source N] notation
3. If the context doesn't contain enough information, say so
4. Be concise but comprehensive

Context:
{context}

Question: {query}

Provide a clear, well-structured answer with citations:"""

    # Call OpenAI API
    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful research assistant that provides accurate answers with proper citations."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,  # Lower temperature for more factual responses
        max_tokens=500
    )

    answer = response.choices[0].message.content

    # Calculate confidence score based on number of sources cited
    cited_sources = len([s for s in retrieved_chunks if f"[Source" in answer])
    confidence = "High" if cited_sources >= 3 else "Medium" if cited_sources >= 1 else "Low"

    # Extract source citations
    citations = []
    for i, chunk in enumerate(retrieved_chunks, 1):
        if f"[Source {i}]" in answer:
            citations.append({
                'source_id': i,
                'filename': chunk['metadata']['original_filename'],
                'relevance_score': chunk['rerank_score']
            })

    return {
        'answer': answer,
        'confidence': confidence,
        'citations': citations,
        'num_sources_used': len(citations),
        'total_sources_available': len(retrieved_chunks)
    }

# Example usage
result = generate_answer_with_citations(query, results)

print(f"\n{'='*80}")
print(f"ANSWER:")
print(f"{'='*80}")
print(result['answer'])
print(f"\n{'='*80}")
print(f"CONFIDENCE: {result['confidence']}")
print(f"SOURCES USED: {result['num_sources_used']}/{result['total_sources_available']}")
print(f"\nCITATIONS:")
for citation in result['citations']:
    print(f"  - {citation['filename']} (relevance: {citation['relevance_score']:.3f})")</code></pre>
            </div>

            <p>
                The answer generation stage transforms retrieved chunks into a coherent response with explicit source attribution. By instructing the model to cite sources and only use provided context, we prevent hallucinations while maintaining natural language quality. Users can verify every claim by checking the cited documents.
            </p>

            <p>
                The complete implementation includes a Streamlit dashboard with drag-and-drop upload, YAML configuration editing, query interface, and analytics. Visit the GitHub repository for the full code, setup instructions, and deployment guide.
            </p>

            <div class="section-divider"></div>

            <h2><i class="fas fa-flag-checkered mr-3"></i>Conclusion</h2>

            <p>
                We have built a complete, production-ready RAG system that solves the document intelligence challenge from end to end. By following the six-stage blueprint‚Äîfrom document ingestion through AI answer generation‚Äîyou now have a framework that handles multiple formats, preserves context through intelligent chunking, optimizes costs through embedding caching, and provides accurate retrieval through two-stage ranking.
            </p>

            <p>
                The key innovations make this system production-ready: YAML-driven configuration lets non-developers adjust strategies, dual-database architecture provides both semantic search and analytics, permanent caching eliminates redundant API costs, and two-stage retrieval balances speed with accuracy. The result is a flexible platform that scales from personal projects to enterprise knowledge bases.
            </p>

            <p>
                For next steps, I recommend starting with the baseline implementation from the GitHub repository, then customizing for your specific use case. Experiment with different chunking strategies in the YAML configs, try alternative embedding models, and tune the retrieval parameters. Most importantly, test with real documents and queries from your domain to validate that the system meets your accuracy and performance requirements.
            </p>

            <div class="analogy-card text-center">
                <h3 class="text-2xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-cyan-400 to-violet-400 mb-4">
                    Ready to Build Your Own RAG System?
                </h3>
                <p class="text-slate-300 mb-6">
                    The complete implementation with Streamlit dashboard, YAML configurations, embedding cache, and two-stage retrieval is available on GitHub. Transform your documents into an AI-powered knowledge base today.
                </p>
                <a href="https://github.com/zubairashfaque/intelligent-rag-system" class="btn-primary">
                    <i class="fab fa-github mr-2"></i>View on GitHub
                </a>
            </div>
        </article>
    </div>

    <!-- Footer -->
    <footer class="bg-slate-900/50 border-t border-slate-700 mt-20 py-12">
        <div class="max-w-7xl mx-auto px-4 text-center">
            <p class="text-slate-400">¬© 2025 Zubair Ashfaque. Built with passion for AI and data science.</p>
            <div class="flex justify-center gap-6 mt-4">
                <a href="https://github.com/zubairashfaque" class="text-slate-400 hover:text-cyan-400 transition">
                    <i class="fab fa-github text-2xl"></i>
                </a>
                <a href="https://linkedin.com/in/zubairashfaque" class="text-slate-400 hover:text-cyan-400 transition">
                    <i class="fab fa-linkedin text-2xl"></i>
                </a>
            </div>
        </div>
    </footer>

    <script>
        function copyCode(button) {
            const codeBlock = button.nextElementSibling;
            const code = codeBlock.textContent;

            navigator.clipboard.writeText(code).then(() => {
                button.innerHTML = '<i class="fas fa-check mr-1"></i>Copied!';
                setTimeout(() => {
                    button.innerHTML = '<i class="far fa-copy mr-1"></i>Copy';
                }, 2000);
            });
        }
    </script>
</body>
</html>